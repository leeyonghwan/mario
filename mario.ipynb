{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lee/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "inpusize[0] 224 256 3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "import dqn\n",
    "\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from gym.scoreboard.registration import add_group\n",
    "from gym.scoreboard.registration import add_task\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "\n",
    "register(\n",
    "     id='SuperMarioBros-1-1-v0',\n",
    "     entry_point='gym.envs.ppaquette_gym_super_mario:MetaSuperMarioBrosEnv',\n",
    ")\n",
    "\n",
    "add_group(\n",
    "     id='ppaquette_gym_super_mario',\n",
    "     name='ppaquette_gym_super_mario',\n",
    "     description='super_mario'\n",
    ")\n",
    "\n",
    "add_task(\n",
    "    id='SuperMarioBros-1-1-v0',\n",
    "    group='ppaquette_gym_super_mario',\n",
    "    summary=\"SuperMarioBros-1-1-v0\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from gym import wrappers\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "env = gym.make('ppaquette/SuperMarioBros-1-1-v0')  \n",
    "\n",
    "frame_history_len=4\n",
    "# Constants defining our neural network\n",
    "#input_size = env.observation_space.shape[0]*env.observation_space.shape[1]*3        #####change input_size - 224*256*3 acquired from ppaquette_gym_super_mario/nes_env.py\n",
    "img_h, img_w, img_c = env.observation_space.shape\n",
    "\n",
    "print(\"inpusize[0]\", env.observation_space.shape[0], env.observation_space.shape[1], env.observation_space.shape[2])\n",
    "\n",
    "input_size = np.array([env.observation_space.shape[0], env.observation_space.shape[1], env.observation_space.shape[2]]) #width*height*3ch#(img_h, img_w, frame_history_len * img_c)\n",
    "# set up placeholders\n",
    "# placeholder for current observation (or state)\n",
    "#obs_t_ph              = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "output_size = 6                                                                     #####meaning of output can be found at ppaquette_gym_super_mario/wrappers/action_space.py\n",
    "\n",
    "_skip = 4\n",
    "dis = 0.9\n",
    "replay_buffer_size = 50000\n",
    "frame_history_len=4\n",
    "\n",
    "# construct the replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_0 = [0, 0, 0, 0, 0, 0]  # NOOP\\n\",\n",
    "a_1 = [1, 0, 0, 0, 0, 0]  #Up\\n\",\n",
    "a_2 = [0, 0, 1, 0, 0, 0] # Down\\n\",\n",
    "a_3 = [0, 1, 0, 0, 0, 0] # Left\\n\"\n",
    "a_4 = [0, 1, 0, 0, 1, 0] # Left + A\\n\",\n",
    "a_5 = [0, 1, 0, 0, 0, 1]  # Left + B\\n\",\n",
    "a_6 = [0, 1, 0, 0, 1, 1]  # Left + A + B\\n\",\n",
    "a_7 = [0, 0, 0, 1, 0, 0] # Right\\n\",\n",
    "a_8 = [0, 0, 0, 1, 1, 0] # Right + A\\n\",\n",
    "a_9 = [0, 0, 0, 1, 0, 1]  # Right + A + B\\n\",\n",
    "a_10 = [0, 0, 0, 1, 1, 1]  #Right + A + B\\n\",\n",
    "a_11 = [0, 0, 0, 0, 1, 0]\n",
    "a_12 = [0, 0, 0, 0, 0, 1]\n",
    "a_13 = [0, 0, 0, 0, 1, 1]\n",
    "\n",
    "def encode_action_rand():\n",
    "    action = random.randrange(0,14)\n",
    "    if action == 0:\n",
    "        return a_0\n",
    "    if action == 1:\n",
    "        return a_1\n",
    "    if action == 2:\n",
    "        return a_2\n",
    "    if action == 3:\n",
    "        return a_3\n",
    "    if action == 4:\n",
    "        return a_4\n",
    "    if action == 5:\n",
    "        return a_5\n",
    "    if action == 6:\n",
    "        return a_6\n",
    "    if action == 7:\n",
    "        return a_7\n",
    "    if action == 8:\n",
    "        return a_8\n",
    "    if action == 9:\n",
    "        return a_9\n",
    "    if action == 10:\n",
    "        return a_10\n",
    "    if action == 11:\n",
    "        return a_11\n",
    "    if action == 12:\n",
    "        return a_12\n",
    "    if action == 13:\n",
    "        return a_13\n",
    "\n",
    "def encode_action( action):\n",
    "    if action == 0:\n",
    "        return a_0\n",
    "    if action == 1:\n",
    "        return a_1\n",
    "    if action == 2:\n",
    "        return a_2\n",
    "    if action == 3:\n",
    "        return a_3\n",
    "    if action == 4:\n",
    "        return a_4\n",
    "    if action == 5:\n",
    "        return a_5\n",
    "    if action == 6:\n",
    "        return a_6\n",
    "    if action == 7:\n",
    "        return a_7\n",
    "    if action == 8:\n",
    "        return a_8\n",
    "    if action == 9:\n",
    "        return a_9\n",
    "    if action == 10:\n",
    "        return a_10\n",
    "    if action == 11:\n",
    "        return a_11\n",
    "    if action == 12:\n",
    "        return a_12\n",
    "    if action == 13:\n",
    "        return a_13\n",
    "\n",
    "# def decode_action( input):\n",
    "#     if a_0 == input:\n",
    "#         return 0\n",
    "#     if a_1 == input:\n",
    "#         return 1\n",
    "#     if a_2 == input:\n",
    "#         return 2\n",
    "#     if a_3 == input:\n",
    "#         return 3\n",
    "#     if a_4 == input:\n",
    "#         return 4\n",
    "#     if a_5 == input:\n",
    "#         return 5\n",
    "#     if a_6 == input:\n",
    "#         return 6\n",
    "#     if a_7 == input:\n",
    "#         return 7\n",
    "#     if a_8 == input:\n",
    "#         return 8\n",
    "#     if a_9 == input:\n",
    "#         return 9\n",
    "#     if a_10 == input:\n",
    "#         return 10\n",
    "#     if a_11 == input:\n",
    "#         return 11\n",
    "#     if a_12 == input:\n",
    "#         return 12\n",
    "#     if a_13 == input:\n",
    "#         return 13\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ddqn_replay_train(mainDQN, targetDQN, train_batch, l_rate):\n",
    "    '''\n",
    "    Double DQN implementation\n",
    "    :param mainDQN: main DQN\n",
    "    :param targetDQN: target DQN\n",
    "    :param train_batch: minibatch for train\n",
    "    :return: loss\n",
    "    '''\n",
    "\n",
    "    x_stack = np.empty(0).reshape(0, mainDQN.input_size[0]*mainDQN.input_size[1]*mainDQN.input_size[2])\n",
    "    y_stack = np.empty(0).reshape(0, mainDQN.output_size)\n",
    "    action_stack = np.empty(0).reshape(0, 30)\n",
    "\n",
    "    # Get stored information from the buffer\n",
    "    for state, action_seq, action_next_seq, action, reward, next_state, done  in train_batch:\n",
    "        Q = mainDQN.predict(state, action_seq)\n",
    "\n",
    "        # terminal?\n",
    "        if done:\n",
    "            Q[0, action] = reward\n",
    "        else:\n",
    "            # get target from target DQN (Q')\n",
    "            Q[0, action] =  reward + dis * targetDQN.predict(next_state, action_next_seq)[0, np.argmax(mainDQN.predict(next_state, action_next_seq))]\n",
    "\n",
    "        if state is None:\n",
    "            print(\"None State, \", action, \" , \", reward, \" , \", next_state, \" , \", done)\n",
    "        else:\n",
    "            y_stack = np.vstack([y_stack, Q])\n",
    "            x_stack = np.vstack([x_stack, state.reshape(-1, mainDQN.input_size[0]*mainDQN.input_size[1]*mainDQN.input_size[2])])\n",
    "            action_stack = np.vstack([action_stack, np.reshape(action_seq, (-1, 30))])\n",
    "     #   y_stack = np.vstack([y_stack, Q])\n",
    "    #    x_stack = np.vstack( [x_stack, state])\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(x_stack, y_stack, action_stack, l_rate = l_rate)\n",
    "\n",
    "def get_copy_var_ops(*, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "\n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "def bot_play(mainDQN, env=env):\n",
    "    # See our trained network in action\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "def _step(action):\n",
    "    total_reward = 0.0\n",
    "    done = None\n",
    "    _obs_buffer = deque(maxlen=2)\n",
    "    for _ in range(_skip):\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        _obs_buffer.append(obs)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    max_frame = np.max(np.stack(_obs_buffer), axis=0)\n",
    "    return max_frame, total_reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input  15\n",
      "distance  730  current distance  708\n",
      "Episode: 0 steps: 996\n",
      "distance  730  current distance  709\n",
      "Episode: 1 steps: 978\n",
      "distance  738  current distance  660\n",
      "Episode: 2 steps: 1007\n",
      "distance  898  current distance  854\n",
      "Episode: 3 steps: 1002\n",
      "distance  898  current distance  746\n",
      "Episode: 4 steps: 971\n",
      "distance  898  current distance  673\n",
      "Episode: 5 steps: 1007\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea62a7b5c913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ea62a7b5c913>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mstep_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.5/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym-super-mario/ppaquette_gym_super_mario/nes_env.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_info_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch_fceux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym-super-mario/ppaquette_gym_super_mario/nes_env.py\u001b[0m in \u001b[0;36m_launch_fceux\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrom_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'>/dev/null'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2>/dev/null'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'&'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[1;32m    674\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1226\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1229\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    max_episodes = 5000\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque()\n",
    "    # saver = tf.train.Saver()\n",
    "    \n",
    "    max_distance = 0\n",
    "    batch_size = 32;\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = dqn.DQN(sess, input_size, output_size, name=\"target\")\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        print(\" input \", input_size[2])\n",
    "        #initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            e = 1. / ((episode / 20) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                replay_buffer.store_frame(state)\n",
    "                \n",
    "                if np.random.rand(1) < e or state is None or state.size == 1:           #####why does this happen?\n",
    "                    output = encode_action_rand()#env.action_space.sample()\n",
    "                    action = output\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    #action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                    output = encode_action_rand()#env.action_space.sample()\n",
    "                    action = output\n",
    "                    \n",
    "#                     output = mainDQN.predict(acc_state, output_seq) \n",
    "#                     output = np.argmax(output) #####flatten it and change it as a list .flatten().tolist()\n",
    "#                     action = encode_action( output)\n",
    "\n",
    "#                    for i in range(len(output)):                                        #####the action list has to have only integer 1 or 0\n",
    "#                        if action[i] > 0.5 :\n",
    "#                            action[i] = 1                                               #####integer 1 only, no 1.0\n",
    "#                        else:\n",
    "#                            action[i] = 0                                               #####integer 0 only, no 0.0\n",
    "\n",
    "#                next_action = OutputToAction3(output)\n",
    "#                print(\"random action:\", output) \n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, info = _step(action)\n",
    "                \n",
    "                replay_buffer.store_effect(action, reward, done)\n",
    "                \n",
    "                if info['distance'] > max_distance:\n",
    "                    max_distance = info['distance']\n",
    "                \n",
    "                if done: # Penalty\n",
    "                    reward = -100\n",
    "                    print(\"distance \", max_distance, \" current distance \",info['distance'] )\n",
    "                    break     \n",
    "                    \n",
    "                state = next_state            \n",
    "                step_count += 1\n",
    "                \n",
    "                if step_count > 10000:   # Good enough. Let's move on\n",
    "                    break\n",
    "                    \n",
    "            print(\"Episode: {} steps: {}\".format(episode, step_count))\n",
    "            continue\n",
    "            if step_count > 10000:\n",
    "                pass\n",
    "                # break\n",
    "        \n",
    "            if episode % 10 == 1: # train every 10 episode\n",
    "                # Get a random batch of experiences\n",
    "\n",
    "                obs_batch, act_batch, rew_batch, obs_tp1_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                loss, _ = ddqn_replay_train(mainDQN, targetDQN, minibatch, l_rate)\n",
    "                  \n",
    "                        \n",
    "                print(\"Loss: \", loss)\n",
    "                # copy q_net -> target_net\n",
    "                sess.run(copy_ops)\n",
    "\n",
    "        # See our trained bot in action\n",
    "        env2 = wrappers.Monitor(env, 'gym-results', force=True)\n",
    "        \n",
    "        #save_path = saver.save(sess, \"./mario_model_1\")\n",
    "    \n",
    "        for i in range(200):\n",
    "            bot_play(mainDQN, env=env2)\n",
    "\n",
    "        env2.close()\n",
    "        # gym.upload(\"gym-results\", api_key=\"sk_VT2wPcSSOylnlPORltmQ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
